start-dfs.sh      # start the hadoop daemons
start-master.sh   # start the spark cluster master 
start-slaves.sh   # start the spark cluster slaves

# download the related twitter jars
spark-shell --packages org.apache.bahir:spark-streaming-twitter_2.11:2.3.2

# copy the related twitter jars to $SPARK_HOME/jars
cp .ivy2/jars/* $SPARK_HOME/jars

# restart the spark-shell using the spark standalone cluster
spark-shell --master spark://spks:7077

# have the kfk-producer compiled to be used as producer to kafka
spark-shell --master spark://spkm:7077 --jars KfkProducer/target/scala-2.11/kfk-producer_2.11-1.0.0.jar

---------------------------

import org.apache.spark._
import org.apache.spark.streaming._
import org.apache.spark.streaming.twitter._
import twitter4j.Status
import twitter4j.auth.OAuthAuthorization
import twitter4j.conf.ConfigurationBuilder

val builder = new ConfigurationBuilder()
builder.setDebugEnabled(true)
builder.setOAuthConsumerKey("<ConsumerKey>")
builder.setOAuthConsumerSecret("<ConsumerSecret>")
builder.setOAuthAccessToken("<AccessToken>")
builder.setOAuthAccessTokenSecret("<AccessTokenSecret>")

val auth = new OAuthAuthorization(builder.build)

val filters = Array("covid-19", "coronavirus")

val ssc = new StreamingContext(sc,Seconds(10))
val twitterStream = TwitterUtils.createStream(ssc, Some(auth), filters )

val tweets = twitterStream.filter(tweet => tweet.getLang.equals("en") || tweet.getLang.equals("")).map(_.getText()).map(_.replaceAll("/[^A-Za-z0-9 ]/", "")).map(_.replaceAll("/", "")).map(_.replaceAll("RT.+?(?=\\s)\\s", "")).map(_.replaceAll("https([^\\s]+).*", ""))

import kfkProducerWrapper.KafkaProducerWrapper

tweets.foreachRDD( rdd  => {
  rdd.foreachPartition( iter  => {
    KafkaProducerWrapper.brokerList = "ubkafka:9092"
    val producer = KafkaProducerWrapper.instance
    iter.foreach( vvalue =>
       producer.send("metric", "key", vvalue) )
    })
})

sc.setCheckpointDir("/home/hadoop/checkpoint")

ssc.start()